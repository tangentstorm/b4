#+title: wejal bootstrap

* A Grammar Interpreter
:PROPERTIES:
:TS:       <2016-04-15 10:52AM>
:ID:       7zli6i3147h0
:END:

Our goal is to bootstrap an interpreter for a new programming language.
We would like to build it incrementally from the ground up.

Rather than writing a parser by hand, we would like to model the language syntax and semantics as data, and pass this data to a generic /grammar interpreter/.

We will build the grammar interpreter incrementally, with test cases.

For starters, we will focus on simple regular expressions:, much like python's built-in =re= module.

#+name: @doctests.matcher
#+begin_src python
>>> m = Matcher(Alt([Lit("x"), Lit("y")])) # like re.compile("x|y")
>>> m.match('xyz')                         # should match "x" at position 0
Match(txt='x', pos=0)
>>> m.match("zyx")                         # should fail to match
FAIL
#+end_src

Obviously, the concise syntax provided by the =re= module is nicer than building pattern structures by hand. We will address this soon by bootstrapping our own parser for grammar definitions. In the meantime, we will build our rule definitions by building data structures with python's =namedtuple=.

* Grammar Combinators
:PROPERTIES:
:TS:       <2015-01-18 07:56AM>
:ID:       9906u111jqg0
:END:

Here are our data types for modeling grammar definitions. Since we are using =namedtuple=, there is no actual behavior associated with them. They're just constructors we can manually compose to create data structures.

#+name: @imports
#+begin_src python :session :results none
  from collections import namedtuple
#+end_src
#+name: @code
#+begin_src python :session :results none
  def T(tag, doc, args):
      """Creates a new tuple type."""
      res = namedtuple(tag, args)
      if doc: res.__doc__+=' : '+doc
      return res

  def TB(tag, doc, args=['body']): return T(tag, doc, args)
  def TI(tag, doc, args=['item']): return T(tag, doc, args)
  def TN(tag, doc, args=['name']): return T(tag, doc, args)
  def T2(tag, doc, args=['name','body']): return T(tag, doc, args)

  Gram = T('Gram', 'contains grammar rules (may inherit from `bases`).',
           ['name', 'bases', 'doc', 'body'])
  Def = T('Def', 'define a named rule.', ['name','body'])
  Ref = TN('Ref', 'refer to (invoke) a named rule')

  Any = T('Any', 'match anything', [])
  Not = TI('Not', 'fail if the pattern would match, but do not consume')
  Skip = TI('Skip', 'match the pattern, but hide it from other rules')

  Emp = T('Emp', 'empty pattern (always matches)', [])
  Emp = Emp() # since it doesn't need arguments

  Lit = TI('Lit', 'match literal character (using ==)')
  Str = TI('Str', 'match a string of literals')
  Seq = TB('Seq', 'match a sequence of patterns')
  Grp = TB('Grp', 'same as Seq, but renders in parentheses')
  Alt = TB('Alt', 'match any of the alternatives')
  Rep = TI('Rep', 'match 1 or more repetitions.')
  Opt = TI('Opt', 'match 0 or 1 repetitions.')
  Orp = TI('Orp', 'match 0 or more repetitions.')

  Tok = TI('Tok', 'match a token of a given type')
  Var = T2('Var', 'save matched string in a variable.')
  Val = TN('Val', 'match against the saved value.')
  New = TI('New', 'build a new class/tuple instance')
  Arg = TB('Arg', 'pass matched data as arg to containing "New"')

#+end_src

* Generic dispatch
:PROPERTIES:
:TS:       <2015-01-18 11:36AM>
:ID:       w0bhd8b1jqg0
:END:

Since we will be composing instances of these types into trees, and want our system to interpret the types differently, we need a way to map each type to an appropriate handler.

We will imlement this using a base class that simply maps each =namedtuple= type to a method with the same name (and a given prefix).

In other words, when we see an =Alt= node in the tree, we can use this to automatically invoke a method in our interpreter class called =match_Alt=.

#+name: @imports
#+begin_src python :sesson :results none
from warnings import warn
#+end_src

#+name: @code
#+begin_src python :session :results none

  def node_type(node):
      return node.__class__.__name__

  class Dispatcher:
      """Provides a simple generic dispatch mechanism based on method names"""

      def _find_handler(self, prefix, node):
          """(prefix, namedtuple) -> callable"""
          return getattr(self, '_'.join([prefix, node_type(node)]), self._unhandled)

      def _unhandled(self, node, *a, **kw):
          """Warn about unrecognized node types. (Just for development.)"""
          raise ValueError("no handler found for %s" % node_type(node))

      def dispatch(self, prefix, node, *a, **kw):
          """Find and invoke a handler for the given node."""
          h = self._find_handler(prefix, node)
          return h(node, *a, **kw)

#+end_src

* Input cursor
:PROPERTIES:
:TS:       <2015-01-22 05:51AM>
:ID:       m3udu291oqg0
:END:

We also need to keep track of where we are in the input sequence.
The following helper class will do the work for us:

#+name: @code
#+begin_src python :session :results none

  class Cursor:

      def __init__(self, seq:[any]):
          self.seq = seq    # sequence (probably a string)
          self.val = None   # current value
          self.pos = -1     # current position
          self.fwd()

      def fwd(self)->any:
          """Move forward in the sequence and return the next item."""
          end = len(self.seq)
          self.pos = min(self.pos+1, end)
          self.val = None if self.pos == end else self.seq[self.pos] 
          return self

      def at_end(self)->bool:
          """Are we at the end of the sequence?"""
          return self.val is None

#+end_src

* Data structure for parse results.
:PROPERTIES:
:TS:       <2015-01-22 05:58AM>
:ID:       x88gff91oqg0
:END:

Matching should either produce:

... A match object, which stores the matched text, and its position within the input:

#+name: @code
#+begin_src python :session :results none

  Match = namedtuple("Match", ['txt', 'pos'])
  Match.__doc__ = "Match Result"

#+end_src

... Or, a special constant called =FAIL=:

#+name: @code
#+begin_src python :session :results none
  class Fail:
      """Value to indicate failure."""
      def __repr__(self):
          return "FAIL"
  FAIL = Fail()
#+end_src

We are also going to use a namedtuple to represent the match state at any given time.
This state includes the match result, a cursor marking the position in the string, and an environment (which we will use later on for storing state). 

#+name: @code
#+begin_src python :session :results none

  class M(namedtuple("M", ['val', 'cur', 'env'])):
      """Internal Match State"""

      @property
      def matched(self):
          return self.val is not FAIL

#+end_src

* Simple pattern matching.
:PROPERTIES:
:TS:       <2016-04-15 11:15AM>
:ID:       yba9ij4147h0
:END:

Now we can start building the matcher. First we will set up the =Dispatch= stuff:  

#+name: @code
#+begin_src python :session :results none

  class Matcher(Dispatcher):
      """
      A simple matcher for regular languages.
      <<@doctests.matcher>>
      """

      def __init__(self, node):
          self.root = node

      def _match(self, node, cur, env):
          """returns a match state tuple (the `M` class)"""
          return self.dispatch('match', node, cur, env)

      def match(self, s:str):
          cur = Cursor(s)
          env = {}
          return self._match(self.root, cur, env).val

  <<@Matcher.changes>>

#+end_src

Now we are ready to implement the handlers for our initial example:

The simplest case is comparison against a single literal character (=Lit=):

#+name: @code
#+begin_src python :session :results none

  # class Matcher:

      def match_Lit(self, node, cur, env):
          return (M(Match(cur.val, cur.pos), cur.fwd(), env) if cur.val == node.item
                  else M(FAIL, cur, env))
#+end_src


For =Alt=, we just try matching each alternative, in sequence.

Note that only the /first/ matching pattern is returned.

#+name: @code
#+begin_src python :session :results none

  # class Matcher:

      def match_Alt(self, node, cur, env):
          for item in node.body:
              m = self._match(item, cur, env)
              if m.matched: return m
          return m # last failure

#+end_src

At this point, our original example using =Alt([ Lit('x'), Lit('y') ])= works as advertised.

* Regular Expressions
:PROPERTIES:
:TS:       <2016-04-15 02:43PM>
:ID:       9u58i7e147h0
:END:

With =Lit= and =Alt= out of the way, only need a few more cases to allow full regular expressions:

=Emp= matches the empty string. It takes no arguments, and always succeeds:

#+name: @doctests.matcher
#+begin_src python
>>> Matcher(Emp).match("hello")
Match(txt='', pos=0)
#+end_src

=Seq= takes a list of patterns and matches all of them in sequence.

#+name: @doctests.matcher
#+begin_src python
>>> m = Matcher(Seq([Lit("a"), Alt([Lit("a"), Lit("b")])]))
>>> m.match("ab")
Match(txt='ab', pos=0)
>>> m.match("ac")
FAIL
#+end_src

As a special case, =Str= matches a string of literals:

#+name: @doctests.matcher
#+begin_src python
>>> Matcher(Str("hello")).match("hello")
Match(txt='hello', pos=0)
#+end_src

=Rep= matches one or more repetitions of a pattern. It works like =+= in regular expressions.

#+name: @doctests.matcher
#+begin_src python
>>> Matcher(Rep(Lit("a"))).match("aaabbbccc")
Match(txt='aaa', pos=0)
#+end_src

=Opt= indicates that a match is optional. =Opt(x)= is equivalent to =Alt([x, Emp])=. It works like =?= in regular expressions.

#+name: @doctests.matcher
#+begin_src python
>>> m = Matcher(Opt(Lit("a")))
>>> m.match("abc")
Match(txt='a', pos=0)
>>> m.match("xyz")
Match(txt='', pos=0)
#+end_src

=Orp(x)= is shorthand for =Opt(Rep(x))=, and works like =*= in regular expressions.

#+name: @doctests.matcher
#+begin_src python
>>> m = Matcher(Orp(Lit("a")))
>>> m.match("aaabc")
Match(txt='aaa', pos=0)
>>> m.match("xyz")
Match(txt='', pos=0)
#+end_src

If you prefer, you could treat =Orp= as the more primitive operation, and =Rep(x)= as sugar for =Seq([x, Orp(x)])=, but the following implementation uses the rules above:

#+name: @code
#+begin_src python :session :results none

  # class Matcher:

      def match_Emp(self, node, cur, env):
          return M(Match("", cur.pos), cur, env)

      def _join(self, matches):
          """helper to join match results for Seq and Str"""
          if matches is FAIL: return FAIL
          else: return Match(''.join(v.txt for v in matches), matches[0].pos)

      def match_Seq(self, node, cur, env):
          vals = []
          for item in node.body:
              res = self._match(item, cur, env)
              if res.val is FAIL: return M(FAIL, res.cur, env)
              else:
                  val, cur, env = res
                  vals.append(val)
          return M(self._join(vals), res.cur, env)

      def match_Str(self, node, cur, env):
          return self._match(Seq([Lit(c) for c in node.item]), cur, env)

      def match_Rep(self, node, cur, env):
          vals = []
          while True:
              res = self._match(node.item, cur, env)
              if res.val is FAIL: break
              else:
                  val, cur, env = res
                  vals.append(val)
          return M(self._join(vals or FAIL), cur, env)

      def match_Opt(self, node, cur, env):
          return self._match(Alt([node.item, Emp]), cur, env)

      def match_Orp(self, node, cur, env):
          return self._match(Opt(Rep(node.item)), cur, env)

#+end_src

Most modern regular expression engines support additions like groups and backreferences. We will diverge a bit here, though, because we are interested in writing full parsers, with mutually recursive named rules.

* Tokenization
:PROPERTIES:
:TS:       <2016-04-17 01:17PM>
:ID:       ua87gur077h0
:END:
While not strictly required, it's traditional to break parsing up into two phases: the first pass scans through the text and breaks it up into tokens, a process called tokenization or lexing.The second pass parses the stream of tokens and (at least conceptually) constructs a tree-like structure. 

Our version of tokens will just be tuples strings, tagged with rule names and match positions:

#+name: @doctests.scanner
#+begin_src python
>>> s = Scanner([("a+", Rep(Lit("a"))), ("b+", Rep(Lit("b")))])
>>> s.scan("abaabb")
[('a', 'a+', 0), ('b', 'b+', 1), ('aa', 'a+', 2), ('bb', 'b+', 4)]

>>> s.scan("a b   \t bb a")  # whitespace is ignored by default.
[('a', 'a+', 0), ('b', 'b+', 2), ('bb', 'b+', 8), ('a', 'a+',  11)]
#+end_src


Our implementation is incredibly naive: it just keeps looping through the list of rules and trying to match each one.

Later on, we can improve the performance by compiling the rules into a state machine, but we will stick with something simple while we're bootstrapping the rest of the system:

#+name: @code
#+begin_src python :session :results none

  class Scanner:

      def __init__(self, rules: [(str, namedtuple)]):
          self.order = [rule[0] for rule in rules]  # test rules in given order
          self.rules = dict(rules)
          # default whitespace handler:
          if '_' not in rules:
              self.order.insert(0, '_')
              self.rules['_'] = Alt([Lit(chr(i)) for i in range(33)])

      def gen_tokens(self, txt):
          cur = Cursor(txt)
          env = {}
          matcher = Matcher(Emp)
          while not cur.at_end():
              for rule in self.order:
                  m = matcher._match(self.rules[rule], cur, env)
                  if m.matched:
                      match, cur, env = m
                      if rule != '_':
                          yield (match.txt, rule, match.pos)
                      break
                  else: continue
              else:
                  raise ValueError("unrecognized character at position %i : '%s'"
                                   % (cur.pos, cur.val))

      def scan(self, txt):
          return list(self.gen_tokens(txt))

#+end_src

* EBNF
:PROPERTIES:
:TS:       <2015-01-18 12:51PM>
:ID:       bd6hv400kqg0
:END:
We are about to extend the simple string matcher to a full parsing system.

One thing we would like to be able to parse is a nicer syntax for building grammars.

There are various languages for writing grammars. We will use one called 'EBNF', which is an acronym for /Extended Backus-Naur Form/).

Here's a grammar for EBNF written in EBNF, so we can test the parser.

#+name: ebnf
#+begin_src prolog
main = { rule } .
rule = IDENT "=" expr "." .
expr = term { "|" term } .
term = factor { factor } .
factor = IDENT | STRING | "{" expr "}" | "[" expr "]" | "(" expr ")" .
#+end_src

This definition is adapted from [[http://www.inf.ethz.ch/personal/wirth/CompilerConstruction/index.html][Compiler Construction]] by Niklaus Wirth (who invented EBNF, as well as Pascal, Modula, Oberon, and a variety of other languages).

It is self describing. The ={...}= syntax corresponds to =Orp(Seq(...))= in our world. The =|= is placed between alternatives, and the characters in quotes correspond to =Lit=.

The =[...]= syntax defined in the =factor= rule isn't actually used by this grammar, but it corresponds to =Opt(...)=. The =(...)= syntax corresponds to =Seq(...)=. These can of course be nested inside each other to arbitrary depths.

The lower case names correspond to rule definitions and references to those rules. These are =Def= and =Ref= in our system -- we'll be covering those soon.

The upper case names refer to token types. A =STRING= is just a sequence of characters between double quotes, and an =IDENT= just means a sequence of english letters.

I placed the definition code in a block of its own so it would be syntax highlighted, but for python it should be inside a string:

#+name: @code
#+begin_src python :session :results none
ebnf_src = '''\
<<ebnf>>
'''
#+end_src

Our next major goal will be to parse grammars like these. First, we will manually create a tokenizer for this language, then we will extend the matcher with =Def= and =Ref= and the ability to match tokens rather than just strings. Then we will manually translate the above EBNF definition into a data structure built that we can pass to the grammar interpreter.

* A Bootstrap tokenizer.
:PROPERTIES:
:TS:       <2016-04-17 03:48PM>
:ID:       0zsa4ty077h0
:END:

With the tools we have now, there's really not much work to define a scanner for EBNF:

#+name: @code
#+begin_src python :session :results none

  ECHR, SQ, DQ = ['\\', "'", '"']
  LETTER = Alt([Lit(ch) for ch in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'])
  STRCHR = Alt([Seq([Lit(ECHR), Alt([ Lit(ECHR), Lit(DQ) ])]),
                Alt([Lit(ch) for ch in map(chr, range(32,127)) if ch not in '"\\'])])

  ebnf_sc = Scanner([(ch, Lit(ch)) for ch in "{([=|.])}"]
                    + [('IDENT', Rep(LETTER)),
                       ('STRING', Alt([Seq([Lit(DQ), Rep(STRCHR), Lit(DQ)])]))])
#+end_src

Here's how to use it:

#+name: @doctests.module
#+begin_src python
>>> ebnf_sc.scan('x = A | b')
[('x', 'IDENT', 0), ('=', '=', 2), ('A', 'IDENT', 4), ('|', '|', 6), ('b', 'IDENT', 8)]

#+end_src

* Parsing
:PROPERTIES:
:TS:       <2016-04-20 05:41PM>
:ID:       3r1lhs11b7h0
:END:

To parse recursive grammars (where patterns can be nested inside each other to arbitrary depths) there's not too much more we need to do. We just need to supply our interpreter with a bunch of named rules, and then be able to tell it when to match those rules. Unlike a scanner, where the choice of pattern that gets matched depends on the input (as if each pattern were a child of an =Alt=), the parser starts with a top-level rule that it attempts to match, and follows references to other rules only when they appear in the patterns.

* Named Rules
:PROPERTIES:
:TS:       <2016-04-20 06:04PM>
:ID:       fy19ru21b7h0
:END:

For our purposes, a =Parser= is just a generic =Matcher= that adds named rules that can refer to each other recursively, and which operates on a stream of tokens rather than just a string.

Therefore, we are going to subclass =Matcher=. However, rather than adding support for =Def= and =Ref= only in the subclass, I'm going to push them up into =Matcher=. That way, =Matcher= will have support for named rules matching on strings, and the only difference we'll see in =Parser= is that it operates on tokens.

We need one addition to the =Matcher= constructor:

#+name: @Matcher.changes
#+begin_src python

  # update to class Matcher

      def __init__(self, node):
          self.root = node
          self.defs = {}

#+end_src


Now we can create and refer to rules:

#+name: @Matcher.changes
#+begin_src python :session :results none

  # addition to class Matcher

      def match_Def(self, node, cur, env):
          self.defs[node.name] = node.body
          return self.match_Emp(node, cur, env)

      def match_Ref(self, node, cur, env):
          # pass in fresh World, then discard changes
          res = self._match(self.defs[node.name], cur, {})
          return M(res.val, res.cur, env) if res.matched else M(FAIL, cur, env)

#+end_src

Here's how it works:

#+name: @doctests.module
#+begin_src python
>>> Matcher( Seq([Def("x", Lit("n")), Ref("x")])).match("n")
Match(txt='n', pos=0)
#+end_src




* Parsing Tokens
:PROPERTIES:
:TS:       <2016-04-20 06:14PM>
:ID:       wgld8b31b7h0
:END:

Now the =Parser= class can just focus on the work of turning a stream of tokens into a tree.

One of the main differences is that we are now operating on tokens. This means we can have =Lit= match a token rather than a character, and therefore we no longer have any need for =Str=.

However, there are two ways we might want to match a token: by its content (for example, a literal keyword like python's =def=) or by its kind (for example, any string). We will continue to use =Lit= for an exact match on the content, and introduce =Tok= for a match on the token type.

Further, we need to override =_join= so that we build nested lists of tokens, rather than simply concatenating strings.

#+name: @code
#+begin_src python

  class Parser(Matcher):

      # cur.val will be a tuple(match:str, kind:str, pos:int)

      # @override
      def match_Lit(self, node, cur, env):
          """match a token on its content"""
          return (M(Match(cur.val, cur.pos), cur.fwd(), env) if cur.val and cur.val[0] == node.item
                  else M(FAIL, cur, env))

      # exact same thing, but match token type instead of content
      def match_Tok(self, node, cur, env):
          """match a token on its type"""
          return (M(Match(cur.val, cur.pos), cur.fwd(), env) if cur.val and cur.val[1] == node.item
                  else M(FAIL, cur, env))

      def match_Str(self, node, cur, env):
          raise TypeError("Str nodes make no sense in a grammar. Consider Lit, Tok or Seq.")

      def _join(self, matches):
          """helper to join match results for Seq"""
          # Seq already gives us either FAIL or a list, so really we're just turning
          # off the filter that Matcher used. However, since tokens already have position
          # information, we can take the opportunity to remove the Match() wrapper, and
          # just return a list:
          if matches is FAIL: return FAIL
          return [m if isinstance(m, list) else m.txt
                  for m in matches if isinstance(m, list) or m.txt != '']

#+end_src

Here it is in action:

#+name: @doctests.module
#+begin_src python
>>> Parser( Seq([Def("x", Lit("n")), Rep(Ref("x")) ])).match([("n", "t1", 0), ("n", "t2", 1)])
[[('n', 't', 0), ('n', 't', 1)]]

>>> Parser( Seq([Def("x", Tok("t")), Rep(Ref("x")) ])).match([("abc", "t", 0), ("xyz", "t", 3)])
[[('abc', 't', 0), ('xyz', 't', 3)]]
#+end_src


* The EBNF grammar
:PROPERTIES:
:TS:       <2016-04-20 07:18PM>
:ID:       crld9961b7h0
:END:

We now have everything we need to interpret a grammar for EBNF:

#+name: @code
#+begin_src python :session :results none
  ebnf = Seq([
      Def('rule', Seq([Tok('IDENT'), Lit('='), Ref('expr'), Lit('.') ])),
      Def('expr', Seq([ Ref('term'), Orp(Seq([Lit('|'), Ref('term') ])) ])),
      Def('term', Rep(Ref('factor'))),
      Def('factor', Alt([Tok('IDENT'), Ref('rep'), Ref('opt'), Ref('grp') ])),
      Def('rep', Seq([Lit('{'), Ref('expr'), Lit('}')])),  # 'x*'
      Def('opt', Seq([Lit('['), Ref('expr'), Lit(']')])),  # 'x?'
      Def('grp', Seq([Lit('('), Ref('expr'), Lit(')')])),  # '(x)'

      # top level rule:
      Orp(Ref('rule'))
  ])
#+end_src

Here's how it might be used:

#+begin_src python
>>> import pprint
>>> pprint.pprint(Parser(ebnf).match(ebnf_sc.scan("ruleC = ruleA | {ruleB} .")))
[[[('ruleC', 'IDENT', 0),
   ('=', '=', 6),
   [[('ruleA', 'IDENT', 8)],
    [[('|', '|', 14),
      [[('{', '{', 16), [[('ruleB', 'IDENT', 17)]], ('}', '}', 22)]]]]],
   ('.', '.', 24)]]]
#+end_src



* TODO ---- clean up below here ----


* Minor cleanups
:PROPERTIES:
:TS:       <2016-04-20 07:28PM>
:ID:       b9gizp61b7h0
:END:

We can get rid of some of the clutter in our grammar definition by allowing python strings and lists as shorthand for =Lit= and =Seq= nodes, respectively:

#+name: @Matcher.changes
#+begin_src python :session :results none

  # addition to class Matcher

      def match_str(self, node, cur, env):
          """just some sugar so that  'abc' <-> Lit('abc')"""
          return self._match(Lit(node), cur, env)

      def match_list(self, node, cur, env):
          """just some sugar so that  [...] <-> Seq(...)"""
          return self._match(Seq(node), cur, env)
#+end_src



* TODO =Gram= node handler.
:PROPERTIES:
:TS:       <2016-04-20 07:31PM>
:ID:       58rc5w61b7h0
:END:



Adding one more handler lets us wrap the grammar definition in a nice wrapper object, with special support for a =main= rule:

#+name: @code
#+begin_src python :session :results none
  _ebnf = Gram('ebnf', [], "ebnf meta-grammar (for parsing grammars)", [
      Def('main', Orp(Ref('rule'))),
      Def('rule',  [Tok('IDENT'), '=', Ref('expr'), '.']),
      Def('expr',  [Ref('term'), Orp(['|', Ref('term')]) ]),
      Def('term',  Rep(Ref('factor'))),
      Def('factor', Alt([Tok('IDENT'), Ref('rep'), Ref('opt'), Ref('grp') ])),
      Def('rep', ['{', Ref('expr'), '}']),  # 'x*'
      Def('opt', ['[', Ref('expr'), ']']),  # 'x?'
      Def('grp', ['(', Ref('expr'), ')']),  # '(x)'
  ])
#+end_src




* Strategy
:PROPERTIES:
:TS:       <2015-01-18 10:25AM>
:ID:       nrogjy71jqg0
:END:

The idea here is to manually construct a data structure (an abstract syntax tree) that describes a meta-grammar.

The meta-grammar describes whatever nice clean syntax we'd /like/ to use for creating grammars in the future.

Building these trees by hand can get messy, though, so we'll stick with a simple syntax for this first round, and then use /that/ to implement something better later.

Our first step is to define some types that we can use to tag the different parts of the tree. Each type represents the some feature of our pattern matching system.

* Manually build a base grammar to provide generic tokenization.
:PROPERTIES:
:TS:       <2015-01-18 10:10AM>
:ID:       9d0f2971jqg0
:END:
#+name: @imports
#+begin_src python :session :results none
  import string
#+end_src
#+name: @code
#+begin_src python :session :results none

  base = Gram('base', [], "rules common to all grammars", [
      Def('main', Orp('token')),
      Def('token',Seq([Skip(Orp(Ref('space'))),
                    Alt([Ref('STRING'), Ref('NUMBER'),
                         Ref('IDENT'), Ref('DELIM'),
                         Rep(Not(Ref('space')))])])),
      Def('space', Orp('White')),
      # character classes:
      Def('White', Alt([chr(c) for c in range(33)])),
      Def('Upper', Alt(list(string.ascii_uppercase))),
      Def('Lower', Alt(list(string.ascii_lowercase))),
      Def('Alpha', Alt([Ref('Lower'), Ref('Upper')])),
      Def('Under', Lit('_')),
      Def('Neg', Lit('-')),
      Def('Digit', Alt([Lit(c) for c in string.digits])),
      Def('Hexit', Alt([Ref('Digit')]+[Lit(c) for c in 'abcdefABCDEF'])),
      Def('Alnum', Alt([Ref('Under'), Ref('Alpha'), Ref('Digit')])),
      # simple patterns:
      Def('IDENT', Seq([Alt([Ref('Under'),Ref('Alpha')]), Orp(Ref('Alnum'))])),
      Def('NUMBER',Seq([Opt(Ref('Neg')), Rep(Ref('Digit')),
                     Orp([Ref('Under'),
                          Ref('Digit'),Ref('Digit'),Ref('Digit')])])),
      Def('STRING', Seq([Lit(DQ), Rep(Ref('STRCHR')), Lit(DQ)])),
      Def('STRCHR', Alt([Seq([Lit(ECHR), Alt([ Lit(ECHR), Lit(DQ) ])]),
                         Not(DQ) ])),
      Def('DELIM', Alt(list('(){}[]'))),
  ])
#+end_src

* Now define the bootstrap grammar to parse EBNF grammar definitions.
:PROPERTIES:
:TS:       <2015-01-18 08:27AM>
:ID:       7o9j7i21jqg0
:END:


* Worlds for backtracking.
:PROPERTIES:
:TS:       <2015-01-18 12:59PM>
:ID:       u8s6vh00kqg0
:END:

A world is a context for holding changes, similar to a working copy in a version control system. The idea is that any time we might need to backtrack (any time an =Alt= node is encountered), we'll fork a new world, and changes we make are done to the world object. This way, if the match ultimately fails, we can rewind the side effects.

This ability is common in prototype-based langugaes like Self and JavaScript (though it isn't necessarily commonly /used/). The name 'World' and the idea of applying it to parsing comes from Alex Warth's [[http://www.tinlizzie.org/ometa/][OMeta]] dissertation.

It's easy to make a python class that works this way: we just override =__getattr__= (for the =x.a= syntax), and =__getitem__= (for the =x[a]= syntax) so that they delegate to a prototype object when there's no local value defined.

Since we do /not/ override the corresponding =__setitem__= and =__setattr__= methods, any assignment made to an attribute or item of the world will affect the local object, leaving the prototype's value unchanged.

It's very much like what happens when overriding methods in a subclass, except it happens for individual objects rather than classes, and it happens dynamically at runtime.

#+name: @code
#+begin_src python :session :results none

  HOME = {} # arbitrary dictionary object

  class World(dict):

      def __init__(self, proto=HOME):
          super(World, self).__init__()
          self.proto = proto

      def __getattr__(self, name):
          # called when attribute has no local definition.
          return getattr(self.proto, name)

      def __getitem__(self, key):
          if key in self.keys(): return super(World, self)[key]
          else: return self.proto[key]

      def changed(self, key, val):
          """Forks a new world, with one key changed."""
          res = World(self)
          res[key] = val
          return res

#+end_src

* Grammar Interpreter
:PROPERTIES:
:TS:       <2015-01-18 12:28PM>
:ID:       n0pcnnd1jqg0
:END:

We will assume for now that we have the entire string in memory.

#+name: @code
#+begin_src python :session :results none

  class Grin(Dispatcher):
      """Grammar Interpreter"""

      def __init__(self, root):
          super(Grin,self).__init__(root)
          self.init(root)

      def parse(self, src):
          self.env = World()
          self.src, self.pos, self.ch = src, 0, ''
          self.page, self.line, self.col = 0, 0, 0

      <<@methods>>
#+end_src

* OUTPUT wejalboot.py
:PROPERTIES:
:TS:       <2015-01-18 12:38PM>
:ID:       npdbb4e1jqg0
:END:

And now we can put the whole thing together:

#+begin_src python :session :tangle "wejalboot.py" :noweb yes
  """
  <<@doctests.module>>
  """
  <<@imports>>
  <<@code>>
  if __name__=="__main__":
      print(Grin(ebnf).parse(ebnf_src))
#+end_src

If we try to run this now, here's what we'll get:

#+begin_src org
=wejalboot.py:92:= *UserWarning: no handler for init_Gram*
  ~yield warn('no handler for tag: %s' % node.__class__.__name__)~
/None/
#+end_src

So now our job is to go back and fill in a handler method for each node until it's able to walk the whole tree.

* Inference Rules
:PROPERTIES:
:TS:       <2015-01-22 06:01AM>
:ID:       yg99mk91oqg0
:END:

These were translated from the sequent notation in Warth's Ometa paper.

#+name: @methods
#+begin_src python :session :results none

  # (inside  `class Grin`...)


  def match_Not(self, node, cur, env):
      res = self.match(node.item, cur, env)
      if res.val is FAIL: return (None, cur, res[1])
      else: return (FAIL, res[1])

  def match_Var(self, node, cur, env):
      res = self.match(node.item, cur, env)
      if res.val is FAIL: return res
      else: return (res.val, cur, env.changed(node.name, res.val))

  def match_Act(self, node, cur, env):
      raise NotImplementedError('no semantic actions yet.')

  def match_Box(self, node, cur, env):
      raise NotImplementedError('no tree matching yet.')

#+end_src

* Compilation step.
:PROPERTIES:
:TS:       <2015-01-18 02:10PM>
:ID:       ks01bt30kqg0
:END:

#+name: @methods
#+begin_src python :session :results none

  # (still inside  `class Grin`...)
  def init(self, node):
      return self.dispatch('init', node)

  def init_Gram(self, node):
      self.defs = {}
      for child in node.body: self.init(child)

  def init_Def(self, node):
      self.defs[node.name] = node

#+end_src

* TODO credits
:PROPERTIES:
:TS:       <2015-01-22 08:13AM>
:ID:       hnv0l310pqg0
:END:
- grammar rules (and the 'world' concept) are adapted from Alessandro Warth's [[http://tinlizzie.org/ometa/][Ometa]] system.

